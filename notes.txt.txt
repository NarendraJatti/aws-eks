https://github.com/kodekloudhub/amazon-elastic-kubernetes-service-course/blob/main/docs/deploy.md
https://github.com/kodekloudhub/amazon-elastic-kubernetes-service-course/blob/main/docs/nodes.md
https://github.com/kodekloudhub/jenkins-project/blob/main/eksClusterCreation.md


showcreds


latest>>https://github.com/kodekloudhub/certified-kubernetes-administrator-course/blob/master/managed-clusters/eks/console/docs/01-sign-in.md
describe short ds

kubectl apply --validate=false -f https://github.com/cert-manager/cert-manager/releases/download/v1.11.0/cert-manager.yaml


https://github.com/aws-ia/terraform-aws-eks-blueprints

kubectl get deployment -n kube-system aws-load-balancer-controller

aws eks describe-cluster --region us-east-1 --name demo-eks --query cluster.status 

jenkins(ec2) with eks>>https://www.youtube.com/watch?v=eKiA6pRg0Qc >>important
Running on Jenkins in /var/lib/jenkins/workspace/eks-push: This means that Jenkins is executing the pipeline within the directory /var/lib/jenkins/workspace/eks-push, which is the working directory for the job named eks-push. This is where Jenkins stores files related to the job execution, including the scripts it runs, logs, and any files that may be generated during the pipeline execution.

enkins jobs are often executed as the jenkins user: When Jenkins runs the pipeline, it typically does so as the jenkins user on the system. Therefore, any AWS configuration needs to be accessible to this user. The home directory for the jenkins user on most systems is /var/lib/jenkins.

Why /var/lib/jenkins: When you run aws configure or use the AWS CLI, the credentials are stored in ~/.aws/credentials and ~/.aws/config within the home directory of the user running the command. In Jenkins, this path is typically /var/lib/jenkins/.aws/. So, if you run aws eks update-kubeconfig, Jenkins needs access to the AWS credentials and configuration in this path.

When to Provide a kubeconfig File as a Secret in Jenkins:
There are situations where you might prefer or need to provide a kubeconfig file as a secret in Jenkins:

Jenkins Agent/Server Does Not Have AWS Access:

If Jenkins does not have the AWS CLI configured (i.e., no aws configure or no role permissions to interact with EKS), you may need to manually provide the kubeconfig file as a secret. This kubeconfig would allow Jenkins to access the Kubernetes cluster without relying on the aws eks update-kubeconfig command.
Security Constraints:

If your organization does not allow direct access to AWS from Jenkins for security reasons, you may need to store the kubeconfig file as a secret in Jenkins (using the Credentials plugin) and inject it into the pipeline during job execution.
Predefined Kubernetes Clusters:

If you're dealing with clusters that are not managed by AWS EKS or if the kubeconfig is static and predefined, you can store the kubeconfig file securely in Jenkins as a secret text credential.
Avoiding Frequent kubeconfig Updates:

In some cases, especially with immutable environments, you might not want to run aws eks update-kubeconfig every time your pipeline runs. Instead, you might create the kubeconfig file once, store it as a secret, and reuse it.

Why Your aws configure Worked:
In your case, the aws eks update-kubeconfig command worked fine because:

Jenkins had access to AWS credentials (either via aws configure or an IAM role attached to the Jenkins node).
The aws eks update-kubeconfig command generates the kubeconfig dynamically, making it easier and more flexible than managing kubeconfig files manually.
If Jenkins has appropriate AWS permissions, this method is generally preferred because it:

Automatically manages the kubeconfig.
Reflects the current state of the EKS cluster.
Avoids the need to handle sensitive kubeconfig files directly in Jenkins.


Built-In Node
Mark this node temporarily offline
This is the Jenkins controller's built-in node. Builds running on this node will execute on the same system and as the same user as the Jenkins controller. This is appropriate e.g. for special jobs performing backups, but in general you should run builds on agents. Learn more about distributed builds.


links
=======
roles>https://www.youtube.com/watch?v=6COvT1Zu9o0
cicd>https://www.youtube.com/watch?v=tkYzg8HRK4o

tools for eks
==============
1)eksctl(CloudFormation stack)
2)eks demo(wrapper on eksctl),https://github.com/awslabs/eksdemo
3)aws iam authenticator for k8,https://github.com/kubernetes-sigs/aws-iam-authenticator

kubectl config current-context
kubectl config get-contexts

kubectl get roles,rolebindings --all-namespaces
kubectl get clusterroles,clusterrolebindings


aws sts get-caller-identity

aws iam simulate-principal-policy --policy-source-arn arn:aws:iam::533267440735:user/demo-user --action-names eks:DescribeCluster --region us-east-1

kubectl create role naren-no-pod-create \
  --verb=list,get,delete \
  --resource=pods \
  --namespace=default

kubectl create rolebinding naren-no-pod-create-binding \
  --role=naren-no-pod-create \
  --user=naren \
  --namespace=default

kubectl create clusterrolebinding naren-no-pod-create-binding \
  --clusterrole=naren-no-pod-create \
  --user=naren


kubectl auth can-i create pods --as=naren

kubectl run nginx --image=nginx --as=naren

kubectl exec -it <nginx-pod-name> -- curl http://localhost:80

kubectl exec -it nginx-release-nginx-app-77d5bc9947-25lv4 -- curl http://localhost:80
kubectl run tmp-shell --rm -i --tty --image busybox -- /bin/sh
wget -qO- http://nginx-release-nginx-app.default.svc.cluster.local:80

kubectl get componentstatuses

kubectl port-forward svc/argocd-server 9000:80 --address 0.0.0.0 -n argocd

argocd with eks >>
==============
argocd cli 
https://medium.com/@chauhanhimani512/install-argocd-on-the-eks-cluster-and-configure-sync-with-the-github-manifest-repository-9e3d62e1c093

~ $ export ARGOCD_SERVER=`kubectl get svc argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`



helm install aws-load-balancer-controller eks/aws-load-balancer-controller \
  --set clusterName= demo-eks \
  --set serviceAccount.create=false \
  --set region=us-east-1 \
  --set vpcId=vpc-0ae8ac2b4ed41e741 \
  --set serviceAccount.name=aws-load-balancer-controller \
  -n kube-system

  important>https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html

https://github.com/devops4solutions/kubernetes-code/blob/main/nginx/service_lb.yaml

kubectl run nginx-pod --image=nginx --port=80

helm charts with eks
=====================
https://www.youtube.com/watch?v=lLLEmrr9uxQ


$ history
    ls
    terraform_version=$(curl -s https://checkpoint-api.hashicorp.com/v1/check/terraform | jq -r -M '.current_version')
    curl -O "https://releases.hashicorp.com/terraform/${terraform_version}/terraform_${terraform_version}_linux_amd64.zip"
    unzip terraform_${terraform_version}_linux_amd64.zip
    mkdir -p ~/bin
    mv terraform ~/bin/
    terraform version
    git clone https://github.com/kodekloudhub/amazon-elastic-kubernetes-service-course
    
   cd amazon-elastic-kubernetes-service-course/eks


   
   #.\check-environment.ps1
   source check-environment.sh
   terraform init
   terraform plan
   terraform apply
   curl http://localhost:8080
   kubectl describe svc nginx-pod
   
eks $ 

important>
We will use the NGINX Ingress Controller instead of the AWS ALB controller.



kubectl port-forward pod/nginx-pod 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
^C~ $ kubectl port-forward pod/nginx-pod 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80
Handling connection for 8080
^C~ $ history
    1  ls
    2  aws eks update-kubeconfig --region us-east-1 --name demo-eks
    3  cd /home/cloudshell-user/.kube/
    4  ls
    5  cat config 
    6  kubectl get contexts
    7  kubectl get context
    8  kubectl get nodes
    9  kubectl config current-context
   10  kubectl config get-contexts
   11  cd
   12  curl -O https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/aws-auth-cm.yaml
   13  vi aws-auth-cm.yaml
   14  k get nodes
   15  kubectl get nodes
   16  kubectl apply -f aws-auth-cm.yaml
   17  kubectl get nodes
   18  kubectl get po
   19  vim nginx-pod.yaml
   20  kubectl apply -f nginx-pod.yaml
   21  vim nginx-pod.yaml
   22  kubectl run nginx-pod --image=nginx --port=80
   23  alias k = kubectl
   24  k get po
   25  kubectl get po
   26  kubectl expose pod nginx-pod --type=LoadBalancer --port=80 --target-port=80
   27  kubectl get svc nginx-pod
   28  curl http://a371149ee0b974863a0707f468acbca8-43760578.us-east-1.elb.amazonaws.com
   29  kubectl get svc nginx-pod
   30  curl http://a371149ee0b974863a0707f468acbca8-43760578.us-east-1.elb.amazonaws.com
   31  kubectl describe pod nginx-pod
   32  kubectl port-forward pod/nginx-pod 8080:80
   33  history
~ $ 







==============================



amazon-elastic-kubernetes-service-course  bin  LICENSE.txt  terraform_1.10.5_linux_amd64.zip
~ $ aws eks update-kubeconfig --region us-east-1 --name demo-eks
Added new context arn:aws:eks:us-east-1:533267440735:cluster/demo-eks to /home/cloudshell-user/.kube/config
~ $ cd /home/cloudshell-user/.kube/
.kube $ ls
config
.kube $ cat config 
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJUThzSWNIaVFOV013RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRBeE1qWXhNek00TUROYUZ3MHpOVEF4TWpReE16UXpNRE5hTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURRaHVLdFdGY1NhUkJ4VGxnem9YVklmdWlabnA4ZVFZOXE0MEtvOGNEWGQxYmFmTk1OeXduZ2JFUUQKYm1LT2NuWXEzTnNabk1QYVcwa3dqL0had0xjTHdpRzZPRkpCU3JnbjF0eFRaV3JDRzVqRXF6dFVyQ3dqemdHYQpkSVcyVjl4c0hWMytnUzA1Q2tMa1N2MXRlalo0OWJzV2tBTEJvSHZYdlZreWQ5Umx1cDlrMGZDWXNtZmN2aGpaCnJ1L2F4NHBNL3Q4R3pBUFFndkpJOTUzZldFMFZnVXNxUnhzZFVIS1RkakhNTDF2dm16eHVrOG4yODVBYkp1dVIKZVFqMkgxTmdDRDJiYXM2UzBQelNPVEhxWklEZHBaZ0FDK3B4WXVKeEZaci9pZGtaczliTWkxV050ZTQxVkJaNApFUDhGLzNmY3o5RGdBd2dlWlV3STNUNlF3cnJ6QWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJRT3JsSndjSG9JYnYzN05xNEZnQWpaZ25tM3pEQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQzd2cVpHU0dQdApJUlFack1USnp5ZU0yNWxEaXp6MkdmSjF3YkhIdVVBZzFsNG1LVGIwUTYvQUlRYldRbDZCeDYwTXBBUXRDMWN5Ckwvc0VWQTU2c1JGenR6MVNBMXJnekgxOFBSOXJrZFJOMW5rSEt4dWcrV3E3OS9FRlloc3BiTVBXNnVacUJxNXkKOXBOZGdmRThUajhTUEpaaHFVQThLdkFUTFNRNDhITjNvUnVYK01OL1FjblJRekdmMllKZ09keHhXSVlyd0kwTApXY2w5T1JaakRNUUJ1THYyUjQ0bVl2NGhCSmtKZWRMMVkxanQzT3JRZDU1T2pkRjJlRVAydFZtcUk4K3VoM1ZCCmhXZHVhTm1NVnIzZkZyTFBFb1hlNFNPWGtLQmk5NktZT0lBZmJLOWFncHYwNnpvNXNWam1CVXdlTDM3d1A4ZjEKdG1rU1BlZmtuckk5Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://8881704DFCCBF69F0C39E76C4D074CB4.gr7.us-east-1.eks.amazonaws.com
  name: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
contexts:
- context:
    cluster: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
    user: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
  name: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
current-context: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
kind: Config
preferences: {}
users:
- name: arn:aws:eks:us-east-1:533267440735:cluster/demo-eks
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - us-east-1
      - eks
      - get-token
      - --cluster-name
      - demo-eks
      - --output
      - json
      command: aws
.kube $ 


In Amazon EKS, by default, each pod gets its own IP address from the VPC subnet. If you have many pods, especially in larger clusters, this can quickly consume a lot of IP addresses. To avoid consuming too many IPs and manage your resources more efficiently, you can use Amazon VPC CNI (Container Network Interface) Plugin for Kubernetes with the following strategies:
You can also control how many pods can run on each node by setting the max-pods parameter in the Node Instance Configuration.
eksctl create nodegroup --max-pods=110


eksdemo get vpc
eksdemo get subnets
eksdemo get Network-Interface

Configuring Warm IP Target
You can configure the warm IP target by editing the Amazon VPC CNI plugin configuration.
In AWS EKS (Elastic Kubernetes Service), a warm IP target is related to how ENIs (Elastic Network Interfaces) and IP addresses are pre-allocated for pods running on EC2 instances (worker nodes). This concept is tied to how the Amazon VPC CNI plugin manages networking for pods and optimizes the assignment of IP addresses to minimize delays when new pods are scheduled.
kubectl edit daemonset aws-node -n kube-system
You can configure the warm IP target by editing the aws-node DaemonSet and setting the WARM_IP_TARGET environment variable.


kubectl config view --minify
kubectl get configmap aws-auth -n kube-system -o yaml


docker push 590184029746.dkr.ecr.us-east-1.amazonaws.com/rekhugopal/eksistiodemo:latest

aws ec2 describe-subnets --query "Subnets[*].{SubnetId:SubnetId, Tags:Tags}" | jq

LB controller
===============
apiVersion: apps/v1
kind: Deployment
...
name: aws-load-balancer-controller
namespace: kube-system
spec:
    ...
    template:
        spec:
            containers:
                - args:
                    - --cluster-name=demo-eks ## Update this to demo-eks

  kubectl version --short


curl http://k8s-default-webappco-a7bb02d6e0-21480f425939c60a.elb.us-east-1.amazonaws.com



https://www.youtube.com/watch?v=5XpPiORNy1o&list=PLmPL-f-MSjiIJZZVffcFGRvCtvYHOl92c&index=7


kubectl config view 
kubectl config current-context 
kubectl config use-context context-name 
aws eks update-kubeconfig \
    --name my-eks-cluster

IRSA 
========
IRSA (IAM Roles for Service Accounts) is an AWS feature that allows Kubernetes pods running on Amazon EKS (Elastic Kubernetes Service) to assume IAM roles for accessing AWS resources securely. Instead of giving blanket permissions to all worker nodes in the cluster, IRSA allows you to assign fine-grained AWS IAM permissions to individual pods using Kubernetes service accounts.


CCM (Cloud Controller Manager) in Kubernetes is responsible for interacting with the underlying cloud provider's API to manage cloud-specific resources. It allows Kubernetes to integrate with the infrastructure components provided by cloud platforms like AWS, Azure, GCP, etc.
Yes, the AWS Cloud Controller Manager (CCM) can create Application Load Balancers (ALBs), but it does so via the AWS Load Balancer Controller, which is a more specific implementation for managing ALBs and Network Load Balancers (NLBs).